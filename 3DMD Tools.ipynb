{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import menpo.io as mio\n",
    "#import scipy.io as sio\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import errno\n",
    "#import menpo.shape\n",
    "#%matplotlib inline\n",
    "import re\n",
    "from shutil import copyfile\n",
    "import fnmatch\n",
    "from PIL import Image\n",
    "import json\n",
    "from pprint import pprint\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Export thumbnails for annotation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = '/vol/phoebe/3DMD_SCIENSE_MUSEUM/15_06_2017/'\n",
    "# root = '/media/ap112/My Passport/22_06_2017/'\n",
    "dest='/media/ap112/C4184698184688FE/Dropbox/PostDoc/'\n",
    "\n",
    "num_images=0\n",
    "num_imgs=[]\n",
    "size= 640, 480\n",
    "ilevel=1\n",
    "num_bmp=0\n",
    "num_obj=0\n",
    "\n",
    "# if os.direxists(os.path.join(os.getcwd()), 'new_folder'))\n",
    "for path, dirs, files in os.walk(root):\n",
    "    if ilevel==1:\n",
    "        subjects=dirs\n",
    "        folder_name=path.split('/')[-2]\n",
    "        print(path,dirs,files,'folder name is',folder_name)        \n",
    "    ilevel=ilevel+1\n",
    "    for name in files:\n",
    "        if fnmatch.fnmatch(name, '*.bmp'):\n",
    "            num_images=num_images+1\n",
    "    if 'meshes' in path.split('/')[-1]:\n",
    "        for name in files:\n",
    "#             e = time()\n",
    "            if fnmatch.fnmatch(name, '*.bmp'):\n",
    "                \n",
    "                tmp_name=name.split('/')\n",
    "#                 old_name=new_name[0]\n",
    "                new_name=tmp_name[-1].split('.')\n",
    "                new_path=dest+folder_name+'/'+new_name[0]\n",
    "#                 print(new_path,'Old name is' ,old_name,new_name)\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                im = Image.open(path+'/'+name)\n",
    "                im.thumbnail(size, Image.ANTIALIAS)\n",
    "                \n",
    "                im.save(new_path+'/'+new_name[0]+'.'+new_name[1]+'.jpg',quality=90)\n",
    "                num_bmp=num_bmp+1\n",
    "#                 print(time() - e)\n",
    "            if fnmatch.fnmatch(name, '*.obj'):\n",
    "                num_obj=num_obj+1\n",
    "#             if old_name != new_name[0]:\n",
    "#                 print(old_name+' has just finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Change json files of a session</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_path='/vol/atlas/databases/3DMD_SCIENSE_MUSEUM/'\n",
    "session_name = '04_05_2017'\n",
    "dest_path_jsons='/vol/phoebe/NEW_3DMD/'\n",
    "# /vol/atlas has been mapped to Z:\n",
    "# /vol/phoebe has been mapped to P:\n",
    "# Relative path for the exporting directory, the path that bmps has been exported\n",
    "# as it should be written to json file\n",
    "exportdirectory_path='Z:/databases/3DMD_SCIENSE_MUSEUM'\n",
    "\n",
    "# Absolute paths for calibrations and mstereo.ini\n",
    "# destination root path\n",
    "dest_root_path='P:/NEW_3DMD'\n",
    "calibations_path=''.join([dest_root_path,'/calibrations'])\n",
    "mstereo_path=''.join([dest_root_path,'/Mstereo/mstereo_default.ini']) \n",
    "#  Absolyte path to back up all the old json files,\n",
    "#  old json files are also backed up in the same folder changing just the extension\n",
    "backupjson_path=''.join([dest_path_jsons,session_name,'/backupjson'])\n",
    "\n",
    "\n",
    "num_images=0\n",
    "num_imgs=[]\n",
    "ilevel=1\n",
    "num_copied=0 \n",
    "\n",
    "for path, dirs, files in os.walk(''.join([source_path,session_name])):\n",
    "    if ilevel==1:\n",
    "        subjects=dirs\n",
    "#         folder_name=path.split('/')[-1]\n",
    "        print(path,dirs,files)    \n",
    "    ilevel+=1\n",
    "    for name in files:\n",
    "        if fnmatch.fnmatch(name, '*.json') and len(name.split('.'))==2:\n",
    "            \n",
    "            num_images+=1\n",
    "            source_filename=''.join([path,'/',name])\n",
    "#             dest_filename=''.join([path,'/',name.split('.')[0],'.bak'])\n",
    "            dest_filename=''.join([backupjson_path,'/',name])\n",
    "            if not os.path.exists(os.path.dirname(dest_filename)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(dest_filename))\n",
    "                except OSError as exc: # Guard against race condition\n",
    "                    if exc.errno != errno.EEXIST:\n",
    "                        raise\n",
    "            print(dest_filename)\n",
    "#             if os.path.isfile(source_filename):\n",
    "#                 copyfile(source_filename, dest_filename)\n",
    "#                 copyfile(source_filename, dest_filename2)\n",
    "#                 print(source_filename,' copied to ',dest_filename)\n",
    "#                 print(dest_filename2)\n",
    "#                 num_copied=num_copied+1\n",
    "            with open(path+'/'+name) as data_file:    \n",
    "                data = json.load(data_file)\n",
    "                data_file.close() # Close the JSON file\n",
    "\n",
    "#              old format\n",
    "            exportdirectory=''.join([exportdirectory_path,'/',session_name,'/',name.split('.')[0]]) \n",
    "            if os.path.exists(exportdirectory+'/images'):\n",
    "#             data[\"exportdirectory\"] = ''.join([exportdirectory_path,'/',session_name,'/',name.split('.')[0]]) \n",
    "                data[\"exportdirectory\"] = ''.join([exportdirectory,'/images']) \n",
    "            else:\n",
    "                data[\"exportdirectory\"] = exportdirectory \n",
    "\n",
    "\n",
    "#             print('exportdirectory: ',data[\"exportdirectory\"])\n",
    "            data[\"exportmstereoini\"]=mstereo_path\n",
    "#             print('exportmstereoini: ',data[\"exportmstereoini\"])\n",
    "            data[\"exporttkadir\"]=''.join([calibations_path,'/', data[\"exporttkadir\"].split('/')[-1]])\n",
    "#             print('exporttkadir: ',data[\"exporttkadir\"])\n",
    "            data[\"exportmeshdir\"]=''.join([dest_root_path,'/',session_name,'/',name.split('.')[0],'/meshes'])  #'P:/NEW_3DMD/10_04_2017/00002/\"\n",
    "            data.pop('exportimagedir', None)\n",
    "#             print('exportmeshdir: ',data[\"exportmeshdir\"])\n",
    "#             print(data)\n",
    "## Save our changes to JSON file\n",
    "#             if name=='00001.json':\n",
    "#                   print('00001.json')\n",
    "            with open(dest_filename, 'w') as outfile:  \n",
    "                json.dump(data, outfile)\n",
    "                outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/vol/atlas/databases/3DMD_SCIENSE_MUSEUM/11_05_2017/02075/02075.json') as data_file:\n",
    "     data = json.load(data_file)\n",
    "     data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Functions used to handle Google Spreadsheet</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use creds to create a client to interact with the Google Drive API\n",
    "def read_google_sheets(spreadsheet_name):\n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('client_secret.json', scope)\n",
    "    client = gspread.authorize(creds)\n",
    " \n",
    "    # Find a workbook by name and open the first sheet\n",
    "    # Make sure you use the right name here.\n",
    "    sheet = client.open(spreadsheet_name).sheet1\n",
    " \n",
    "    # Extract and print all of the values\n",
    "    list_of_hashes = sheet.get_all_records()\n",
    "    list_of_hashes=list(filter(lambda x: x['Id']!='', list_of_hashes))\n",
    "    # a = [x for x in new_list if x['Id'] != 250]\n",
    "    return list_of_hashes\n",
    "\n",
    "def searchdate(search_year, people):\n",
    "        return [element for element in people if datetime.strptime(element['DateBirth'],'%d/%m/%Y').year<search_year and element['Ethnicity']=='White' ]\n",
    "\n",
    "def searchidn(idn, people):\n",
    "    return [element for element in people if element['Gender'] == idn]\n",
    "\n",
    "def searchnogender(people):\n",
    "    return [element for element in people if element['Gender'] != 'Male' and element['Gender'] != 'Female']\n",
    "\n",
    "\n",
    "def copy_obj_bmp2jpg_files(root_path,dest_path,list_of_people):\n",
    "    exist_lists=[]\n",
    "    num_copied_subj=0\n",
    "    num_not_found_subj=0\n",
    "    not_found_list=[]\n",
    "    if not os.path.exists(os.path.dirname(dest_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dest_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    for person in list_of_people:\n",
    "    #Transform the date to the one that is used for folders, namely dd_mm_yyyy\n",
    "        if person['Id']<250:\n",
    "            dt=datetime.strptime(person['Timestamp'],'%d/%m/%Y')\n",
    "        else:\n",
    "            dt=datetime.strptime(person['Timestamp'],'%d/%m/%Y %X')\n",
    "        if dt.day<10:\n",
    "            tmpday=''.join(['0',str(dt.day)])\n",
    "        else:\n",
    "            tmpday=str(dt.day)\n",
    "        date_session=''.join([tmpday,'_0',str(dt.month),'_',str(dt.year)])\n",
    "        #Check if  there is zero padding\n",
    "        zeroes_padding=num_zeroes_in_filename(''.join([root_path,date_session]),str(person['Id']))\n",
    "        if zeroes_padding==-1: \n",
    "            print(''.join([date_session,'/',str(person['Id'])]),'cannot be found')\n",
    "            num_not_found_subj+=1\n",
    "            not_found_list.append(person['Id'])\n",
    "            continue\n",
    "        \n",
    "        fileid=str(person['Id']).rjust(zeroes_padding,'0')\n",
    "        source_path=''.join([root_path,date_session,'/',fileid,'/meshes/'])\n",
    "        if os.path.exists(os.path.dirname(source_path)):\n",
    "            filename=''.join([fileid,'.000001'])   \n",
    "            bmpfile=''.join([filename,'.bmp'])\n",
    "            objfile=''.join([filename,'.obj'])\n",
    "            if os.path.isfile(source_path+bmpfile):\n",
    "                          print(source_path+bmpfile,source_path+objfile,' are copying')\n",
    "                          num_copied_subj+=1\n",
    "                          copyfile(source_path+objfile, dest_path+objfile)\n",
    "                          im = Image.open(source_path+bmpfile)\n",
    "                          im.save(dest_path+filename+'.jpg',quality=90)    \n",
    "            exist_lists.append(source_path)\n",
    "    return num_not_found_subj,num_copied_subj,not_found_list\n",
    "\n",
    "def num_zeroes_in_filename(file_path,person_id):\n",
    "    len_id=len(person_id)\n",
    "    if  os.path.exists(''.join([file_path,'/',person_id])):\n",
    "            return len_id\n",
    "    elif  os.path.exists(''.join([file_path,'/0',person_id])):\n",
    "            return len_id+1\n",
    "    elif  os.path.exists(''.join([file_path,'/00',person_id])):\n",
    "            return len_id+2\n",
    "    elif  os.path.exists(''.join([file_path,'/000',person_id])):\n",
    "            return len_id+3\n",
    "    elif  os.path.exists(''.join([file_path,'/0000',person_id])):\n",
    "            return len_id+4\n",
    "    else: \n",
    "            return -1\n",
    "def printonekey(printedkeys,people):\n",
    "    for person in people:\n",
    "        for prkey in printedkeys:\n",
    "            print(person[prkey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_name= \"Copy Museum data\"\n",
    "\n",
    "list_of_hashes=read_google_sheets(spreadsheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nogender=searchnogender(list_of_hashes)\n",
    "print(len(nogender))\n",
    "printonekey(['Id','Gender','Email'],nogender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2325 males 2597 females and 9 others\n",
      "Total:  4931\n"
     ]
    }
   ],
   "source": [
    "males=searchidn('Male', list_of_hashes)\n",
    "females=searchidn('Female', list_of_hashes)\n",
    "no_gender=searchidn('Other', list_of_hashes)\n",
    "print('There are',len(males),'males', len(females),'females and',len(no_gender),'others')\n",
    "print('Total: ',len(no_gender)+len(males)+len(females))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mothers Country': '', 'Fathers Ethnicity': 'White', 'Mothers ethnicity': 'White', 'Id': 206, 'Gender': 'Female', 'Email': 'tomburstow@hotmail.com', 'Country': 'United Kingdom', 'Fathers Country': '', 'Timestamp': '12/04/2017', 'DateBirth': '12/07/2007', 'Language': '', 'Ethnicity': 'White', '': ''}\n"
     ]
    }
   ],
   "source": [
    "print(list_of_hashes[205])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(not_found, copied) = copy_obj_bmp2jpg_files(root_path,dest_path,above60white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4931\n",
      "209\n",
      "469\n",
      "704\n",
      "760\n",
      "1453\n",
      "1450\n",
      "1454\n",
      "1619\n",
      "1744\n",
      "1741\n",
      "1745\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_hashes))\n",
    "i=1\n",
    "tmp_num=list_of_hashes[0]['Id']\n",
    "tmp_list=list_of_hashes[1:]\n",
    "for person in tmp_list:\n",
    "    if  person['Id']-tmp_num!=1:\n",
    "        print( person['Id'])\n",
    "        i+=1\n",
    "    tmp_num=person['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b> Extract obj and jpg files from specific people</b></p>\n",
    "<b> Number 207, 208, 468,703, 759, 1617, 1618 are missing from Google Spreadsheets.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path='/vol/phoebe/3DMD_SCIENCE_MUSEUM/'\n",
    "root_path='/vol/phoebe/NEW_3DMD/'\n",
    "# root_path='/vol/atlas/databases/3DMD_SCIENSE_MUSEUM/'\n",
    "dest_path='/data/Dropbox/Dropbox/Dropbox/females/'\n",
    "# spreadsheet_name= \"Copy Museum data\"\n",
    "#\n",
    "# list_of_hashes=read_google_sheets(spreadsheet_name)\n",
    "print('List has been read')\n",
    "# above60white=searchdate(1967,list_of_hashes)\n",
    "# num_not_found, num_copied = copy_obj_bmp2jpg_files(root_path,dest_path,above60white)\n",
    "\n",
    "# nogender=searchnogender(list_of_hashes)\n",
    "num_not_found, num_copied,list_notfound = copy_obj_bmp2jpg_files(root_path,dest_path,females)\n",
    "print(num_copied,' were copied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Export thumbnails by 4G<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import menpo.io as mio\n",
    "\n",
    "from menpo.visualize import print_progress\n",
    "from pathlib import Path\n",
    "\n",
    "def process_image(im):\n",
    "    im = im.rescale_to_diagonal(700)\n",
    "    subject_id = im.path.stem.split('.')[0]\n",
    "    path =  Path('tmp') / subject_id / im.path.with_suffix('.jpg').name\n",
    "    \n",
    "    try:\n",
    "        mio.export_image(im, path, overwrite=True)\n",
    "    except:\n",
    "        path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        mio.export_image(im, path)\n",
    "\n",
    "# for im in print_progress(mio.import_images('/media/ap112/Seagate Expansion Drive/11_06_2017/**/meshes/*.bmp')):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(4)\n",
    "\n",
    "pool.map_async(process_image, mio.import_images('/media/ap112/Seagate Expansion Drive/11_06_2017/**/meshes/*.bmp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool.map_async()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
